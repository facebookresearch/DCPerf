# Preparing and running spark_standalone benchmark

This benchmarks requires one machine to run the main Spark workload (the compute
node) and one or more machines to provide a total of 3 NVMe SSDs (the storage
nodes) for the compute node to connect as backend storage.

## Kernel support

Please build a version of Linux kernel with NVMe over TCP support enabled by
having the following lines in the kernel config, and then install the kernel
on all the machines to be involved in the benchmark.

```
CONFIG_BLK_DEV_NVME=y
CONFIG_NVME_CORE=y
CONFIG_NVME_MULTIPATH=y
CONFIG_NVME_FABRICS=m
CONFIG_NVME_RDMA=m
CONFIG_NVME_TARGET=m
CONFIG_NVME_TARGET_LOOP=m
CONFIG_NVME_TARGET_RDMA=m
CONFIG_NVME_TARGET_TCP=m
CONFIG_NVME_TCP=m
```

## Note

- There is no need to install non-default version of Python on storage nodes
  because we don't need to run Benchpress CLI on them.

- When running scripts under ./packages/spark_standalone/templates/nvme_tcp,
  please use `alternatives --config python3` to switch Python3 back to the
  system default one, because the approach the scripts installing dependencies
  (e.g. `dnf install python3-*` and `./setup.py install`) will pour the packages
  into the system default Python's library.

## On the storage nodes

We need to first export the data SSDs on the storage nodes.

If you have a single storage node that can provide 3 or more spare data SSDs,
run the following:

```
./packages/spark_standalone/templates/nvme_tcp/setup_nvmet.py exporter setup -n 3 -s 1 -p 1 --real
```

If you have 3 storage nodes and each of them has one spare data SSD, run the
following command on all the three machines:

```
./packages/spark_standalone/templates/nvme_tcp/setup_nvmet.py exporter setup -n 1 -s 1 -p 1 --real
```

If your machines only have a boot drive but the drive has an unused partition that
can be used for storing data (say it's called `/dev/nvme0n1pX`), you can export
the partition instead:

```
./packages/spark_standalone/templates/nvme_tcp/setup_nvmet.py exporter setup -n 1 -s 0 -p X --real
```

When running the commands, it will execute `fdisk` to ask you create partitions.
If the data SSDs are uninitialized, you will need to create a partition in
order for `setup_nvmet.py` to use. If you have already created a partition, you
can skip the step by quitting fdisk.

After each of above command finishes, `setup_nvmet.py` will print out a command
at the end of the output like the following:
```
./packages/spark_standalone/templates/nvme_tcp/setup_nvmet.py importer connect -n 1 -s 1 -i <storage-node-ipaddr> -t nvmet-<storage-node-hostname> --real
```
We will need to execute this command on the compute node to import the data SSD
over network.

## On the compute node

1. Execute the commands generated by `setup_nvmet.py` on all storage nodes.

2. Then we should be able to see a number of additional NVMe devices on the
   compute node by running `lsblk`:

   ```
   NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
   nvme0n1     259:0    0 238.5G  0 disk
   ├─nvme0n1p1 259:1    0   243M  0 part /boot/efi
   ├─nvme0n1p2 259:2    0   488M  0 part /boot
   ├─nvme0n1p3 259:3    0   1.9G  0 part [SWAP]
   └─nvme0n1p4 259:4    0 235.9G  0 part /
   nvme1n1     259:6    0   1.8T  0 disk
   nvme2n1     259:8    0   1.8T  0 disk
   nvme3n1     259:10   0   1.8T  0 disk
   ```

3. Create a RAID-0 array and mount it. Suppose you have 3 SSDs imported now, starting from nvme1n1:

```
./packages/spark_standalone/templates/nvme_tcp/setup_nvmet.py importer mount -n 3 -s 1 --real
```

Then you should see remote SSDs mounted at `/flash23`:

```
[root@compute-node ~/DCPerf]# df -h
Filesystem       Size  Used Avail Use% Mounted on
devtmpfs          32G     0   32G   0% /dev
tmpfs             32G  1.8M   32G   1% /dev/shm
tmpfs             13G   15M   13G   1% /run
/dev/nvme0n1p4   236G   38G  195G  17% /
/dev/nvme0n1p2   465M   90M  347M  21% /boot
/dev/nvme0n1p1   243M  4.4M  239M   2% /boot/efi
/dev/md127       5.3T  261G  5.0T   5% /flash23
```

4. Download dataset

The dataset for this benchmark is hosted in a separate repository
[DCPerf-datasets](https://github.com/facebookresearch/DCPerf-datasets).
Due to its large size, we need to use [git-lfs](https://github.com/git-lfs/git-lfs)
to access the data in it. Below lists the steps to download the dataset:

- Install git-lfs: `dnf install -y git-lfs`
- Clone the dataset repository:
  ```
  cd /flash23
  git clone https://github.com/facebookresearch/DCPerf-datasets
  ```
  `git clone` should automatically download all data included in this repo, but if
  it didn't, please use the following git-lfs commands to download:
  ```
  git lfs track
  git lfs fetch
  ```
- Move the dataset folder `bpc_t93586_s2_synthetic`:
  ```
  mv DCPerf-datasets/bpc_t93586_s2_synthetic ./bpc_t93586_s2_synthetic
  ```

5. Install and run Spark benchmark

Note: please use `alternatives --config python3` to switch python3 to the newer
version you installed for Benchpress

Run the following command on the compute node to install and run
spark_standalone benchmark

```
./benchpress_cli.py install spark_standalone_remote
./benchpress_cli.py run spark_standalone_remote
```

### Reporting

After the benchmark finishing on the compute node, benchpress will output the
results in JSON format like the following:

```
{
  "benchmark_args": [
    "run",
    "--dataset-path /flash23/",
    "--warehouse-dir /flash23/warehouse",
    "--shuffle-dir /flash23/spark_local_dir",
    "--real"
  ],
  "benchmark_desc": "Spark standalone using remote SSDs for database and shuffling point; compute & memory bound as in prod",
  "benchmark_hooks": [],
  "benchmark_name": "spark_standalone_remote",
  "machines": [
    {
      "cpu_architecture": "x86_64",
      "cpu_model": "Intel(R) Xeon(R) Platinum 8321HC CPU @ 1.40GHz",
      "hostname": "<compute-node-hostname>",
      "kernel_version": "5.6.13-05010-g10741cbf0a08",
      "mem_total_kib": "65387096 KiB",
      "num_logical_cpus": "52",
      "os_distro": "centos",
      "os_release_name": "CentOS Stream 8"
    }
  ],
  "metadata": {
    "L1d cache": "32K",
    "L1i cache": "32K",
    "L2 cache": "1024K",
    "L3 cache": "36608K"
  },
  "metrics": {
    "execution_time_test_93586": 1089.5,
    "worker_cores": 36,
    "worker_memory": "42GB"
  },
  "run_id": "7e287f2d",
  "timestamp": 1658971035
}
```

`metrics.test_XXXXX` is the benchmark metric we care about. It represents the execution time.
